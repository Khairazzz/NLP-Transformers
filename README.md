# ğŸ§  Implementasi Arsitektur Transformer dari Nol dengan NumPy

Repositori ini berisi implementasi forward pass dari arsitektur **decoder-only Transformer (GPT-style)** yang dibangun sepenuhnya dari nol menggunakan NumPy.  

---

## âš™ï¸ Fitur

Implementasi ini mencakup semua komponen wajib dari arsitektur Transformer, yaitu:

- âœ… **Token Embedding**  
- âœ… **Positional Encoding (Sinusoidal dan RoPE)**  
- âœ… **Scaled Dot-Product Attention dengan Causal Masking**  
- âœ… **Multi-Head Attention**  
- âœ… **Feed-Forward Network**  
- âœ… **Residual Connection dengan Layer Normalization (Pre-Norm)**  
- âœ… **Lapisan Output dengan Softmax**

Selain itu, implementasi ini juga menyertakan beberapa **fitur bonus** untuk meningkatkan performa dan analisis:

- ğŸŒ€ **Rotary Positional Embedding (RoPE)** â€” alternatif positional encoding yang lebih modern.  
- ğŸ”— **Weight Tying** â€” antara lapisan embedding dan lapisan output untuk efisiensi parameter.  
- ğŸ” **Visualisasi Attention Map** â€” menggunakan Matplotlib dan Seaborn untuk menganalisis bagaimana model memfokuskan perhatiannya.

---

## ğŸ§© Dependensi

Proyek ini hanya memerlukan beberapa library Python standar.  
Pastikan Anda telah menginstal:

- `numpy` â€” untuk semua operasi matematis dan matriks  
- `matplotlib` & `seaborn` â€” untuk visualisasi attention heatmap

---

## â–¶ï¸ Cara Menjalankan Program 

1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/namakamu/nama-repo.git
```

2ï¸âƒ£ Masuk ke Direktori Proyek

```bash
git clone https://github.com/namakamu/nama-repo.git
```

3ï¸âƒ£ Instal semua dependensi dengan menjalankan perintah berikut di terminal Anda:

```bash
pip install numpy matplotlib seaborn
```

4ï¸âƒ£ Jalankan Skrip Python

```bash
python transformer.py
```

